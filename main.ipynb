{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1imySp36b3hTs28zJzof7Qta8Z6nZmBbD",
      "authorship_tag": "ABX9TyNk9bc/UfYKsbxTP5bGx/Rz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AndyMDH/pneumonia_detection_cnn/blob/main/main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CSCK506  End of Module: Pneumonia Detection through Convolutional Neural Network (CNN)\n",
        "\n"
      ],
      "metadata": {
        "id": "sKYV16KhiNal"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Table of Contents\n",
        "1. [Introduction](#section-1)\n",
        "2. [Exploratory Data Analysis](#section-2)\n",
        "3. [Data Preprocessing and Augmentation](#section-3)\n",
        "4. [CNN Architecture Development](#section-4)\n",
        "5. [Model Evaluation](#section-5)\n",
        "6. [Summary & Retrospective](#section-6)\n",
        "7. [References](#section-7)"
      ],
      "metadata": {
        "id": "dpfCeVsEiwvH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Introduction\n",
        "\n",
        "Pneumonia poses a severe threat to human health, being a potentially life-threatening infectious illness that typically affects one or both lungs. It is frequently triggered by bacteria, notably Streptococcus pneumoniae. According to the World Health Organization (WHO), pneumonia is responsible for one in three deaths in India (Varshni et al., 2019). Medical practitioners often rely on X-ray scans to diagnose pneumonia, distinguishing between bacterial and viral types.\n",
        "\n",
        "This Jupyter notebook delves into the realm of automated pneumonia detection using Convolutional Neural Networks (CNNs). Specifically, it addresses the task of training a CNN model to differentiate between healthy lung scans and those afflicted with pneumonia. The dataset utilised for this endeavor is sourced from the Kaggle competition repository, offering a collection of chest X-ray images categorised as pneumonia-positive and normal.\n",
        "\n",
        "\n",
        "**This task involves, but is not limited to:**\n",
        "\n",
        "a. CNN Model Development:\n",
        "\n",
        "- Write code to train a CNN model using the provided dataset.\n",
        "- Objective: Achieve optimal performance in distinguishing between healthy and pneumonia-infected lung images.\n",
        "\n",
        "    - **Key considerations:**\n",
        "      - Define CNN architecture, including convolution-pooling blocks.\n",
        "      - Fine-tune parameters like strides, padding, and activation functions for accuracy.\n",
        "      - Implement strategies to prevent overfitting and ensure model generalization.\n",
        "\n",
        "b. Training and Evaluation:\n",
        "\n",
        "- Train the CNN model using the provided training dataset.\n",
        "Fine-tune hyperparameters using validation data to enhance performance.\n",
        "- Evaluate the model's accuracy using a separate test dataset to validate pneumonia detection in chest X-ray images.\n",
        "\n",
        "This Jupyter Notebook was collaboratively prepared by:\n",
        "\n",
        "- Minh-Dat Andy Ho Huu\n",
        "- Santiago Fernandez Blanco\n",
        "- Ismael Saumtally\n",
        "- Chi Chuen Wan\n",
        "- Chui Yi Wong"
      ],
      "metadata": {
        "id": "Fp0cMtR7i1cn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Dependencies"
      ],
      "metadata": {
        "id": "_znUfAOJvTQN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Standard library imports\n",
        "import logging\n",
        "import os\n",
        "import unicodedata\n",
        "from collections import defaultdict\n",
        "from typing import List, Optional, Set, Tuple\n",
        "from zipfile import ZipFile\n",
        "import random\n",
        "\n",
        "# Related third party imports\n",
        "import numpy as np\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.layers import Dense, Activation, Conv2D, MaxPooling2D, Flatten, Dropout, BatchNormalization\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from sklearn.metrics import precision_recall_curve, roc_curve, accuracy_score, confusion_matrix, precision_score, recall_score\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "# Warnings configuration\n",
        "import warnings"
      ],
      "metadata": {
        "id": "5o3vHSwgsqpM"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we will also define a constant to decide whether to use the GPU (with CUDA specifically) or the CPU. If you don't have a GPU, set this to False."
      ],
      "metadata": {
        "id": "Jfp1WyIGg0Dn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "USE_CUDA = True"
      ],
      "metadata": {
        "id": "qUMXMi43gyK5"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "warnings.filterwarnings(action='ignore',category=DeprecationWarning)\n",
        "warnings.filterwarnings(action='ignore',category=FutureWarning)"
      ],
      "metadata": {
        "id": "Xtp6hDmr4-8Q"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Download Pneumonia Dataset\n",
        "\n",
        "The Corpus can be downloaded here: [Chest X-Ray Images (Pneumonia)](https://www.kaggle.com/datasets/paultimothymooney/chest-xray-pneumonia?resource=download)"
      ],
      "metadata": {
        "id": "V4bmI1Aq5IGw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)"
      ],
      "metadata": {
        "id": "h_sLk2I26QO7"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load Dataset into DataFrame from Google Drive"
      ],
      "metadata": {
        "id": "6AhITM_N9sJV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Defining dataset directory\n",
        "DATASET_DIR = \"/content/drive/MyDrive/Liverpool/CSCK506 Deep Learning/End of Module/archive/chest_xray\"\n",
        "train_dir = DATASET_DIR + \"/train\"\n",
        "val_dir = DATASET_DIR + \"/val\"\n",
        "test_dir = DATASET_DIR + \"/test\"\n",
        "\n",
        "if os.path.exists(train_dir) and os.path.exists(val_dir) and os.path.exists(test_dir):\n",
        "    print(\"Dataset directories exist.\")\n",
        "else:\n",
        "    print(\"One or more dataset directories are not found. Please recheck the file locations.\")"
      ],
      "metadata": {
        "id": "w-pQ_8vFTRCN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db1c47c5-233c-4b40-8ee0-2ff14da77ea2"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Dataset directories exist.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Alternatively**, if you want to run this notebook from your local machine, you can run the code block below to download and unzip the x-ray files from Kaggle."
      ],
      "metadata": {
        "id": "JMbnpximw3KG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# def download_file(url, destination):\n",
        "#     try:\n",
        "#         urllib.request.urlretrieve(url, destination)\n",
        "#         logger.info(f'Downloaded file from {url} to {destination}')\n",
        "#     except Exception as e:\n",
        "#         logger.error(f'Error downloading file: {e}')\n",
        "\n",
        "# def extract_zip(zip_path, extract_path):\n",
        "#     try:\n",
        "#         with ZipFile(zip_path, 'r') as zip_ref:\n",
        "#             zip_ref.extractall(extract_path)\n",
        "#         logger.info(f'Extracted {zip_path} to {extract_path}')\n",
        "#     except Exception as e:\n",
        "#         logger.error(f'Error extracting zip file: {e}')\n",
        "\n",
        "# def create_directory(directory):\n",
        "#     if not os.path.exists(directory):\n",
        "#         os.makedirs(directory)\n",
        "#         logger.info(f'Created directory: {directory}')\n",
        "\n",
        "# DATASET_NAME = 'chest_x_ray'\n",
        "# DATASET_URL = 'https://www.kaggle.com/datasets/paultimothymooney/chest-xray-pneumonia?resource=download'\n",
        "# DATASET_DIR = os.path.join(DATASET_NAME)\n",
        "# DATASET_ZIP = os.path.join(DATASET_DIR, 'archive.zip')\n",
        "\n",
        "# # Check if dataset directory already exists\n",
        "# if os.path.exists(DATASET_DIR):\n",
        "#     print(f'{DATASET_NAME} already exists')\n",
        "# else:\n",
        "#     if os.path.exists(DATASET_ZIP):\n",
        "#         create_directory(DATASET_DIR)\n",
        "#         extract_zip(DATASET_ZIP, CORPUS_DIR)\n",
        "#         os.remove(DATASET_ZIP)\n",
        "#         print(f'{DATASET_URL_NAME} extracted')\n",
        "#     else:\n",
        "#         print(f'To obtain the \"{DATASET_NAME}\" dataset, please follow these steps:')\n",
        "#         print(f'1. Manually download the WikiQA dataset from: {DATASET_URL}')\n",
        "#         print(f'2. Place the downloaded \"archive.zip\" file in the \"{DATASET_DIR}\" folder.')\n",
        "#         print(f'3. Rerun this script after placing the corpus in the correct location.')\n"
      ],
      "metadata": {
        "id": "JO9DTt_9TXxH"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exploratory Data Analysis"
      ],
      "metadata": {
        "id": "iY2sVdjG-PLT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# labels = ['NORMAL', 'PNEUMONIA']"
      ],
      "metadata": {
        "id": "0sGMrpAQ8lEh"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Data Preprocessing\n"
      ],
      "metadata": {
        "id": "1rrwYa1MJCKB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define constants\n",
        "ROTATION_RANGE = 20\n",
        "WIDTH_SHIFT_RANGE = 0.1\n",
        "HEIGHT_SHIFT_RANGE = 0.1\n",
        "HORIZONTAL_FLIP = True\n",
        "FILL_MODE = 'nearest'\n",
        "TARGET_SIZE = (300, 300)\n",
        "RESCALE = 1. / 255.\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "def create_augmented_data_generator(train_dir):\n",
        "    \"\"\"\n",
        "    Create a data generator for training images with augmentation.\n",
        "\n",
        "    Args:\n",
        "        train_dir (str): Path to the directory containing training images.\n",
        "\n",
        "    Returns:\n",
        "        DirectoryIterator: Data generator for training images with augmentation.\n",
        "    \"\"\"\n",
        "    train_datagen = ImageDataGenerator(\n",
        "        rotation_range=ROTATION_RANGE,\n",
        "        width_shift_range=WIDTH_SHIFT_RANGE,\n",
        "        height_shift_range=HEIGHT_SHIFT_RANGE,\n",
        "        horizontal_flip=HORIZONTAL_FLIP,\n",
        "        fill_mode=FILL_MODE,\n",
        "        rescale=RESCALE\n",
        "    )\n",
        "\n",
        "    train_data_generator = train_datagen.flow_from_directory(\n",
        "        train_dir,\n",
        "        target_size=TARGET_SIZE,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        class_mode='binary'\n",
        "    )\n",
        "\n",
        "    return train_data_generator\n",
        "\n",
        "def create_val_data_generator(val_dir):\n",
        "    \"\"\"\n",
        "    Create a data generator for validation images without augmentation.\n",
        "\n",
        "    Args:\n",
        "        val_dir (str): Path to the directory containing validation images.\n",
        "\n",
        "    Returns:\n",
        "        DirectoryIterator: Data generator for validation images without augmentation.\n",
        "    \"\"\"\n",
        "    val_datagen = ImageDataGenerator(rescale=RESCALE)\n",
        "\n",
        "    val_data_generator = val_datagen.flow_from_directory(\n",
        "        val_dir,\n",
        "        target_size=TARGET_SIZE,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        class_mode='binary'\n",
        "    )\n",
        "\n",
        "    return val_data_generator\n",
        "\n",
        "def create_test_data_generator(test_dir):\n",
        "    \"\"\"\n",
        "    Create a data generator for test images without augmentation.\n",
        "\n",
        "    Args:\n",
        "        test_dir (str): Path to the directory containing test images.\n",
        "\n",
        "    Returns:\n",
        "        DirectoryIterator: Data generator for test images without augmentation.\n",
        "    \"\"\"\n",
        "    test_datagen = ImageDataGenerator(rescale=RESCALE)\n",
        "\n",
        "    test_data_generator = test_datagen.flow_from_directory(\n",
        "        test_dir,\n",
        "        target_size=TARGET_SIZE,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        class_mode='binary'\n",
        "    )\n",
        "\n",
        "    return test_data_generator"
      ],
      "metadata": {
        "id": "FiGCGKpAN67G"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# class_indices = {'NORMAL': 0, 'PNEUMONIA': 1}  # Manually defining class indices based on our class labels\n",
        "\n",
        "# print(class_indices)"
      ],
      "metadata": {
        "id": "BnzllJ1-pn-9"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# class_weights_dict = None"
      ],
      "metadata": {
        "id": "eTdSdDYleEAX"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Call the function to create augmented data generator for training set\n",
        "print(\"Creating augmented data generator for training set...\")\n",
        "train_data_generator = create_augmented_data_generator(train_dir)\n",
        "\n",
        "# Call the function to create data generator for validation set\n",
        "print(\"Creating augmented data generator for validation set...\")\n",
        "val_data_generator = create_val_data_generator(val_dir)\n",
        "\n",
        "# Call the function to create data generator for test set\n",
        "print(\"Creating augmented data generator for testing set...\")\n",
        "test_data_generator = create_test_data_generator(test_dir)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wPfkLOXNYdD0",
        "outputId": "c190f3f8-2a7b-4c14-f284-983718371506"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating augmented data generator for training set...\n",
            "Found 5216 images belonging to 2 classes.\n",
            "Creating augmented data generator for validation set...\n",
            "Found 16 images belonging to 2 classes.\n",
            "Creating augmented data generator for testing set...\n",
            "Found 624 images belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_model():\n",
        "    model = tf.keras.models.Sequential([\n",
        "        tf.keras.layers.Conv2D(16, (3, 3), activation='relu', padding='same', strides=(2, 2), input_shape=(300, 300, 3), name=\"First_Convolution\"),\n",
        "        tf.keras.layers.Dropout(rate=0.1, name='Dropout_1'),\n",
        "        tf.keras.layers.MaxPooling2D(2, 2),\n",
        "        tf.keras.layers.Conv2D(32, (3, 3), activation='relu', padding='same', strides=(2, 2), name=\"Second_Convolution\"),\n",
        "        tf.keras.layers.Dropout(rate=0.1, name='Dropout_2'),\n",
        "        tf.keras.layers.MaxPooling2D(2, 2),\n",
        "        tf.keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same', strides=(2, 2), name=\"Third_Convolution\"),\n",
        "        tf.keras.layers.Dropout(rate=0.1, name='Dropout_3'),\n",
        "        tf.keras.layers.MaxPooling2D(2, 2),\n",
        "\n",
        "        # Flatten the results to feed into a DNN\n",
        "        tf.keras.layers.Flatten(),\n",
        "        tf.keras.layers.Dense(512, activation='relu'),\n",
        "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(loss='binary_crossentropy',\n",
        "                  optimizer=tf.keras.optimizers.Adam(learning_rate=0.1),\n",
        "                  metrics=['accuracy'])\n",
        "    return model"
      ],
      "metadata": {
        "id": "_owaHEBrY2Io"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training our model\n",
        "history = model.fit(train_data_generator,\n",
        "                    epochs=10,\n",
        "                    batch_size=1000,\n",
        "                    validation_data=test_data_generator)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_MN4s5qocxc1",
        "outputId": "12741b10-cc3a-4bb9-8482-faf74ce5af8a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            " 80/163 [=============>................] - ETA: 11:21 - loss: 0.4126 - accuracy: 0.8141"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "### References:\n",
        "\n",
        "Varshni, D., Thakral, K., Agarwal, L., Nijhawan, R. and Mittal, A. (2019). Pneumonia Detection Using CNN based Feature Extraction. [online] IEEE Xplore. doi:https://doi.org/10.1109/ICECCT.2019.8869364."
      ],
      "metadata": {
        "id": "gaHb3cx9texv"
      }
    }
  ]
}